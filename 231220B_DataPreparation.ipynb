{
 "cells": [
  {
   "attachments": {
    "NYPLogo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NYPLogo.png](attachment:NYPLogo.png)\n",
    "\n",
    "# IT2311 Assignment - Task 1a: Data Preparation\n",
    "\n",
    "For this task, you are required to perform data understanding to examine your dataset and data cleaning to prepare the data for task 2 and 3.\n",
    "\n",
    "Rename **DataPreparationStarter.ipynb** to **\\<AdminNo\\>_DataPreparation.ipynb** and complete the following sub-tasks:\n",
    "\n",
    "Complete the following sub-tasks:\n",
    "\n",
    "1. **Load Data**: Load your dataset\n",
    "2. **Data Understanding**: Examine your dataset\n",
    "3. **Data Cleaning**: Clean your data and perform all necessary pre-processing tasks\n",
    "4. **Save Data**: Save the cleaned data for next task\n",
    "\n",
    "For each sub-task, perform the necessary steps and **explain the rationale taken for each step in this Jupyter notebook**. \n",
    "\n",
    "**Done by: Clifton Chen Yi, 231220B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Download Packages\n\nWe import all necessary libraries for data loading, exploration, visualization, and NLP preprocessing.\n\n- **pandas**: for data manipulation and analysis\n- **numpy**: for numerical operations\n- **matplotlib / seaborn**: for data visualization\n- **nltk**: for natural language processing (tokenization, stopwords, lemmatization)\n- **re**: for regular expression-based text cleaning\n- **string**: for string constants (punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "print('All libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We load the World Bank project documents dataset from a JSON file. The dataset is stored in JSON Lines format (one JSON object per line), so we use `lines=True`.\n",
    "\n",
    "**Dataset**: `Task_1_TM_world_bank_projects_subset.json`\n",
    "\n",
    "**Citation**: Jordan, Luke S. (2021). *World Bank Project Documents* [Dataset]. Hugging Face. Available at: https://huggingface.co/datasets/lukesjordan/worldbank-project-documents\n",
    "\n",
    "**License**: MIT +no-false-attribs (MITNFA). Any modifications in this notebook were made by Clifton Chen Yi (231220B) and are not endorsed by the original author or the World Bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the World Bank projects dataset from JSON\n",
    "df = pd.read_json('Task_1_TM_world_bank_projects_subset.json', orient='records', lines=True)\n",
    "\n",
    "# Display the first few rows to verify the data loaded correctly\n",
    "print(f'Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "In this section, we perform a comprehensive exploration of the dataset to understand its structure, quality, and characteristics. This is essential before data cleaning to identify issues such as missing values, duplicates, and data distribution patterns.\n",
    "\n",
    "### 2.1 Basic Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape and Data Types\n\nFirst, we examine the dataset dimensions and column data types to understand the overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f'Number of rows: {df.shape[0]}')\n",
    "print(f'Number of columns: {df.shape[1]}')\n",
    "print(f'\\nColumn names: {list(df.columns)}')\n",
    "print()\n",
    "\n",
    "# Data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary\n\nWe look at the descriptive statistics to get an overview of the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for all columns\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First and Last Rows\n\nExamining both ends of the dataset to check for any anomalies at the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "print('=== First 5 rows ===')\n",
    "display(df.head())\n",
    "\n",
    "# Display last 5 rows\n",
    "print('\\n=== Last 5 rows ===')\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis\n",
    "\n",
    "Checking for missing or null values is critical because missing data can affect the quality of topic modelling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage (%)': missing_percentage\n",
    "})\n",
    "print('=== Missing Values Summary ===')\n",
    "print(missing_df)\n",
    "print(f'\\nTotal missing values in dataset: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings (not null but empty) in text columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        empty_count = (df[col].str.strip() == '').sum()\n",
    "        print(f\"Column '{col}': {empty_count} empty strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Duplicate Analysis\n",
    "\n",
    "Duplicates can skew topic modelling results by overrepresenting certain documents. We check for both fully duplicated rows and duplicate project IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for fully duplicated rows\n",
    "full_duplicates = df.duplicated().sum()\n",
    "print(f'Number of fully duplicated rows: {full_duplicates}')\n",
    "\n",
    "# Check for duplicate project_ids\n",
    "duplicate_project_ids = df['project_id'].duplicated().sum()\n",
    "print(f'Number of duplicate project_ids: {duplicate_project_ids}')\n",
    "\n",
    "# Show duplicated project_ids if any\n",
    "if duplicate_project_ids > 0:\n",
    "    dup_ids = df[df['project_id'].duplicated(keep=False)]['project_id'].unique()\n",
    "    print(f'\\nProject IDs that appear more than once ({len(dup_ids)} unique IDs):')\n",
    "    for pid in dup_ids[:10]:  # Show first 10\n",
    "        count = df[df['project_id'] == pid].shape[0]\n",
    "        print(f'  {pid}: {count} occurrences')\n",
    "    if len(dup_ids) > 10:\n",
    "        print(f'  ... and {len(dup_ids) - 10} more')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale**: Since different documents (APPROVAL vs REVIEW) can exist for the same project, duplicate `project_id` values are expected and not an error. We only need to worry about fully identical rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Document Type Distribution\n",
    "\n",
    "Understanding the distribution of document types (APPROVAL vs REVIEW) is important to know if the dataset is balanced and to understand the composition of text we will be modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for document_type\n",
    "doc_type_counts = df['document_type'].value_counts()\n",
    "print('=== Document Type Distribution ===')\n",
    "print(doc_type_counts)\n",
    "print(f'\\nPercentage breakdown:')\n",
    "print(df['document_type'].value_counts(normalize=True).map(lambda x: f'{x:.1%}'))\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "doc_type_counts.plot(kind='bar', ax=axes[0], color=['#2196F3', '#FF9800'], edgecolor='black')\n",
    "axes[0].set_title('Document Type Distribution (Count)', fontsize=12)\n",
    "axes[0].set_xlabel('Document Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "doc_type_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#2196F3', '#FF9800'])\n",
    "axes[1].set_title('Document Type Distribution (Percentage)', fontsize=12)\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text Length Analysis\n",
    "\n",
    "Analyzing the length of document texts helps identify potential outliers (e.g., extremely short or long documents) and understand the typical size of documents in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length columns for analysis\n",
    "df['text_length_chars'] = df['document_text'].str.len()\n",
    "df['text_length_words'] = df['document_text'].str.split().str.len()\n",
    "\n",
    "# Summary statistics for text length\n",
    "print('=== Text Length Statistics (Characters) ===')\n",
    "print(df['text_length_chars'].describe())\n",
    "print(f'\\n=== Text Length Statistics (Words) ===')\n",
    "print(df['text_length_words'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram of character length\n",
    "axes[0, 0].hist(df['text_length_chars'], bins=50, color='#2196F3', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Document Length (Characters)', fontsize=11)\n",
    "axes[0, 0].set_xlabel('Number of Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram of word length\n",
    "axes[0, 1].hist(df['text_length_words'], bins=50, color='#FF9800', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Document Length (Words)', fontsize=11)\n",
    "axes[0, 1].set_xlabel('Number of Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot of character length by document type\n",
    "df.boxplot(column='text_length_chars', by='document_type', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Character Length by Document Type', fontsize=11)\n",
    "axes[1, 0].set_xlabel('Document Type')\n",
    "axes[1, 0].set_ylabel('Number of Characters')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.title('Character Length by Document Type')\n",
    "\n",
    "# Box plot of word length by document type\n",
    "df.boxplot(column='text_length_words', by='document_type', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Word Length by Document Type', fontsize=11)\n",
    "axes[1, 1].set_xlabel('Document Type')\n",
    "axes[1, 1].set_ylabel('Number of Words')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.title('Word Length by Document Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text length statistics by document type\n",
    "print('=== Text Length by Document Type ===')\n",
    "print(df.groupby('document_type')[['text_length_chars', 'text_length_words']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Identify Very Short Documents\n",
    "\n",
    "Very short documents may not contain enough meaningful content for topic modelling. We identify documents that may be too short to contribute useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify very short documents (less than 100 words)\n",
    "short_docs = df[df['text_length_words'] < 100]\n",
    "print(f'Number of documents with fewer than 100 words: {len(short_docs)}')\n",
    "print(f'Percentage of dataset: {len(short_docs)/len(df)*100:.2f}%')\n",
    "\n",
    "if len(short_docs) > 0:\n",
    "    print('\\nSample of short documents:')\n",
    "    for idx, row in short_docs.head(3).iterrows():\n",
    "        print(f\"\\n  Project ID: {row['project_id']} | Type: {row['document_type']} | Words: {row['text_length_words']}\")\n",
    "        print(f\"  Text preview: {row['document_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Sample Text Inspection\n",
    "\n",
    "Examining actual text content helps identify patterns, noise, and potential cleaning needs (e.g., special characters, boilerplate text, formatting artifacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample APPROVAL document text\n",
    "print('=== Sample APPROVAL Document Text (first 1000 chars) ===')\n",
    "approval_sample = df[df['document_type'] == 'APPROVAL']['document_text'].iloc[0]\n",
    "print(approval_sample[:1000])\n",
    "print('\\n' + '='*80)\n",
    "\n",
    "# Inspect sample REVIEW document text\n",
    "print('\\n=== Sample REVIEW Document Text (first 1000 chars) ===')\n",
    "review_sample = df[df['document_type'] == 'REVIEW']['document_text'].iloc[0]\n",
    "print(review_sample[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Project ID Analysis\n",
    "\n",
    "Understanding the distribution of project IDs can reveal whether certain projects have multiple documents and the coverage of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique project IDs\n",
    "unique_projects = df['project_id'].nunique()\n",
    "total_docs = len(df)\n",
    "print(f'Total documents: {total_docs}')\n",
    "print(f'Unique project IDs: {unique_projects}')\n",
    "print(f'Average documents per project: {total_docs/unique_projects:.2f}')\n",
    "\n",
    "# Distribution of documents per project\n",
    "docs_per_project = df.groupby('project_id').size()\n",
    "print(f'\\nDocuments per project distribution:')\n",
    "print(docs_per_project.describe())\n",
    "\n",
    "# Projects with most documents\n",
    "print(f'\\nTop 10 projects by number of documents:')\n",
    "print(docs_per_project.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Key Findings from Data Understanding\n",
    "\n",
    "Based on the exploration above, here are the key findings that will guide our data cleaning:\n",
    "\n",
    "1. **Dataset Structure**: The dataset contains three columns \u2014 `project_id`, `document_text`, and `document_type` \u2014 all stored as text/object types.\n",
    "2. **Missing Values**: We identified the extent of missing/null values and empty strings.\n",
    "3. **Duplicates**: We checked for fully duplicated rows (which should be removed) vs. expected duplicate project IDs (different document types for the same project).\n",
    "4. **Document Types**: The distribution between APPROVAL and REVIEW documents was examined.\n",
    "5. **Text Characteristics**: Document lengths vary significantly. Very short documents may need to be excluded as they may not provide meaningful content for topic modelling.\n",
    "6. **Text Noise**: Raw text contains newline characters (`\\n`), special characters, numbers, extra whitespace, and boilerplate/header text that need to be cleaned for effective topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Based on our data understanding findings, we now clean and preprocess the data to prepare it for topic modelling. The cleaning steps are designed to remove noise while preserving meaningful content.\n",
    "\n",
    "### Cleaning Strategy:\n",
    "1. **Remove duplicate rows** \u2014 to avoid biasing topic models with repeated documents\n",
    "2. **Handle missing values** \u2014 drop rows with missing/empty document text as they cannot be used for modelling\n",
    "3. **Remove very short documents** \u2014 documents that are too short lack sufficient content for meaningful topic extraction\n",
    "4. **Text normalization** \u2014 convert to lowercase for consistency\n",
    "5. **Remove noise from text** \u2014 URLs, email addresses, numbers, special characters, and extra whitespace\n",
    "6. **Tokenization** \u2014 split text into individual words\n",
    "7. **Remove stopwords** \u2014 remove common English words that don't carry topical meaning\n",
    "8. **Lemmatization** \u2014 reduce words to their base form to consolidate vocabulary\n",
    "9. **Remove short tokens** \u2014 single and two-character tokens are typically not meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove Fully Duplicated Rows\n\n**Rationale**: Exact duplicate rows provide no additional information and would give disproportionate weight to certain documents in topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original count for comparison\n",
    "original_count = len(df)\n",
    "\n",
    "# Remove fully duplicated rows\n",
    "df_clean = df.drop_duplicates().copy()\n",
    "removed_duplicates = original_count - len(df_clean)\n",
    "print(f'Rows before: {original_count}')\n",
    "print(f'Duplicates removed: {removed_duplicates}')\n",
    "print(f'Rows after: {len(df_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing and Empty Values\n\n**Rationale**: Rows with missing or empty `document_text` cannot be used for topic modelling since there is no text content to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null document_text\n",
    "before_count = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=['document_text'])\n",
    "print(f'Removed {before_count - len(df_clean)} rows with null document_text')\n",
    "\n",
    "# Remove rows with empty or whitespace-only document_text\n",
    "before_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['document_text'].str.strip().str.len() > 0]\n",
    "print(f'Removed {before_count - len(df_clean)} rows with empty document_text')\n",
    "print(f'Remaining rows: {len(df_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Remove Very Short Documents\n",
    "\n",
    "**Rationale**: A minimum threshold of 50 words is used based on the following considerations:\n",
    "- Documents shorter than 50 words typically contain only headers, metadata, or boilerplate text (as observed in section 2.6) rather than substantive project descriptions.\n",
    "- Topic modelling algorithms (e.g., LDA) require sufficient word co-occurrence data within each document to infer topics reliably; very short documents produce sparse and noisy topic assignments.\n",
    "- The threshold of 50 words balances between retaining as much data as possible and ensuring each document has enough meaningful content for topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate word count after deduplication\n",
    "df_clean['text_length_words'] = df_clean['document_text'].str.split().str.len()\n",
    "\n",
    "before_count = len(df_clean)\n",
    "min_word_threshold = 50\n",
    "df_clean = df_clean[df_clean['text_length_words'] >= min_word_threshold]\n",
    "print(f'Removed {before_count - len(df_clean)} documents with fewer than {min_word_threshold} words')\n",
    "print(f'Remaining rows: {len(df_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Text Preprocessing Pipeline\n",
    "\n",
    "We define a comprehensive text cleaning function that will be applied to all document texts. Each step has a specific rationale:\n",
    "\n",
    "- **Lowercase**: Ensures 'Bank' and 'bank' are treated as the same word\n",
    "- **Remove URLs**: Web links are not meaningful for topic analysis\n",
    "- **Remove email addresses**: Personal information that doesn't contribute to topics\n",
    "- **Remove numbers**: Numeric values (dates, amounts) are generally not useful for topic extraction\n",
    "- **Remove special characters and punctuation**: These add noise to the token vocabulary\n",
    "- **Remove extra whitespace**: Normalizes spacing for clean tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess document text for topic modelling.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Raw document text\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Replace newline characters with spaces\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "print('Applying text cleaning...')\n",
    "df_clean['cleaned_text'] = df_clean['document_text'].apply(clean_text)\n",
    "print('Text cleaning complete.')\n",
    "\n",
    "# Show a sample of cleaned text\n",
    "print('\\n=== Original Text (first 300 chars) ===')\n",
    "print(df_clean['document_text'].iloc[0][:300])\n",
    "print('\\n=== Cleaned Text (first 300 chars) ===')\n",
    "print(df_clean['cleaned_text'].iloc[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Tokenization (Unigrams & Bigrams), Stopword Removal, and Lemmatization\n",
    "\n",
    "**Tokenization**: Splits text into individual words (tokens). We generate both **unigrams** (single words) and **bigrams** (consecutive word pairs) to capture multi-word concepts. For example, the phrase \"water supply\" would produce the unigram tokens `[\"water\", \"supply\"]` and the bigram token `[\"water_supply\"]`.\n",
    "\n",
    "**Rationale for bigrams**: Many meaningful concepts in development documents are multi-word phrases (e.g., \"rural_development\", \"credit_facility\", \"poverty_reduction\"). Using only unigrams would lose these compound meanings. Bigrams allow topic models to discover these richer, more specific themes.\n",
    "\n",
    "**Stopword Removal**: Removes common English words (e.g., \"the\", \"is\", \"and\") that appear frequently but do not convey meaningful topic information. We also add domain-specific stopwords commonly found in World Bank documents.\n",
    "\n",
    "**Lemmatization**: Reduces words to their dictionary base form (e.g., \"projects\" \u2192 \"project\", \"developing\" \u2192 \"develop\"). This consolidates the vocabulary and improves topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add domain-specific stopwords that appear in virtually ALL World Bank documents.\n",
    "# These words (e.g., 'world', 'bank', 'project') are ubiquitous across the entire\n",
    "# corpus and would dominate every topic without providing differentiation between\n",
    "# document subjects. Removing them allows the model to surface more meaningful,\n",
    "# distinguishing themes within the project documents.\n",
    "domain_stopwords = {\n",
    "    'world', 'bank', 'project', 'document', 'report', 'page', \n",
    "    'official', 'use', 'copy', 'file', 'would', 'also', 'may',\n",
    "    'including', 'within', 'one', 'two', 'three', 'new', 'first',\n",
    "    'could', 'shall', 'must', 'might', 'yet', 'since', 'however'\n",
    "}\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# Configurable minimum token length (tokens shorter than this are removed)\n",
    "MIN_TOKEN_LENGTH = 2\n",
    "\n",
    "print(f'Total stopwords (English + domain-specific): {len(stop_words)}')\n",
    "print(f'Minimum token length: {MIN_TOKEN_LENGTH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_process(text):\n",
    "    \"\"\"\n",
    "    Tokenize text, remove stopwords, lemmatize, remove short tokens,\n",
    "    and generate both unigrams and bigrams.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Cleaned text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of unigram and bigram tokens, e.g. ['water', 'supply', 'water_supply']\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    # Tokenize into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > MIN_TOKEN_LENGTH]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Generate bigrams from the filtered/lemmatized unigrams\n",
    "    bigram_tokens = ['_'.join(bg) for bg in ngrams(tokens, 2)]\n",
    "    \n",
    "    # Combine unigrams and bigrams into a single list\n",
    "    all_tokens = tokens + bigram_tokens\n",
    "    \n",
    "    return all_tokens\n",
    "\n",
    "# Apply tokenization and processing\n",
    "print('Applying tokenization (unigrams + bigrams), stopword removal, and lemmatization...')\n",
    "print('(This may take a few minutes depending on dataset size)')\n",
    "df_clean['processed_text'] = df_clean['cleaned_text'].apply(tokenize_and_process)\n",
    "print('Processing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of the full pipeline result\n",
    "print('=== Full Pipeline Example ===')\n",
    "print(f'\\nOriginal text (first 200 chars):')\n",
    "print(df_clean['document_text'].iloc[0][:200])\n",
    "print(f'\\nCleaned text (first 200 chars):')\n",
    "print(df_clean['cleaned_text'].iloc[0][:200])\n",
    "print(f'\\nProcessed text (first 20 tokens):')\n",
    "print(df_clean['processed_text'].iloc[0][:20])\n",
    "print(f'\\nTotal tokens in first document: {len(df_clean[\"processed_text\"].iloc[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Remove Documents with Empty Processed Text\n\n**Rationale**: After stopword removal and filtering, some documents may have no remaining meaningful tokens. These cannot contribute to topic modelling and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove documents where processed_text is empty\n",
    "before_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['processed_text'].apply(len) > 0]\n",
    "print(f'Removed {before_count - len(df_clean)} documents with empty processed text')\n",
    "print(f'Final document count: {len(df_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Data Cleaning Summary and Validation\n",
    "\n",
    "We summarize all cleaning steps and verify the final dataset quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset summary\n",
    "print('='*60)\n",
    "print('DATA CLEANING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Original dataset size:  {original_count} documents')\n",
    "print(f'Final dataset size:     {len(df_clean)} documents')\n",
    "print(f'Total documents removed: {original_count - len(df_clean)}')\n",
    "print(f'Retention rate: {len(df_clean)/original_count*100:.1f}%')\n",
    "print()\n",
    "print('=== Final Dataset Info ===')\n",
    "print(f'Columns: {list(df_clean.columns)}')\n",
    "print(f'\\nDocument type distribution:')\n",
    "print(df_clean['document_type'].value_counts())\n",
    "print(f'\\nMissing values:')\n",
    "print(df_clean[['project_id', 'document_text', 'document_type', 'processed_text']].isnull().sum())\n",
    "print(f'\\nProcessed text token count statistics:')\n",
    "df_clean['processed_token_count'] = df_clean['processed_text'].apply(len)\n",
    "print(df_clean['processed_token_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the processed text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Processed text token count distribution\n",
    "axes[0].hist(df_clean['processed_token_count'], bins=50, color='#4CAF50', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Processed Text Length (Tokens)', fontsize=11)\n",
    "axes[0].set_xlabel('Number of Tokens')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Processed text token count by document type\n",
    "df_clean.boxplot(column='processed_token_count', by='document_type', ax=axes[1])\n",
    "axes[1].set_title('Processed Text Length by Document Type', fontsize=11)\n",
    "axes[1].set_xlabel('Document Type')\n",
    "axes[1].set_ylabel('Number of Tokens')\n",
    "plt.sca(axes[1])\n",
    "plt.title('Processed Text Length by Document Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the final cleaned dataset\n",
    "print('=== Sample of Final Cleaned Dataset ===')\n",
    "df_clean[['project_id', 'document_type', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data\n",
    "\n",
    "We save the cleaned dataset for use in the next task (Topic Modelling). We save:\n",
    "- The essential columns (`project_id`, `document_type`, `processed_text`) as a CSV file\n",
    "- The full cleaned DataFrame as a JSON file for reference\n",
    "\n",
    "**Rationale**: CSV format is chosen for the primary output as it is widely compatible with topic modelling libraries. The JSON file is saved as a standard JSON array (not JSON Lines) to ensure compatibility when loading with `json.load()` or `pd.read_json()`. The `processed_text` column contains both unigram and bigram tokens ready for direct use in topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as json_lib\n",
    "\n",
    "# Select columns to save\n",
    "output_df = df_clean[['project_id', 'document_type', 'processed_text']].copy()\n",
    "\n",
    "# Save as CSV (convert list to JSON array string for CSV compatibility)\n",
    "output_csv_path = 'cleaned_data.csv'\n",
    "output_df_csv = output_df.copy()\n",
    "output_df_csv['processed_text'] = output_df_csv['processed_text'].apply(json_lib.dumps)\n",
    "output_df_csv.to_csv(output_csv_path, index=False)\n",
    "print(f'Cleaned data saved to: {output_csv_path}')\n",
    "print(f'Shape: {output_df.shape}')\n",
    "\n",
    "# Save as JSON (standard JSON array format - lists are natively supported)\n",
    "output_json_path = 'cleaned_data.json'\n",
    "df_clean[['project_id', 'document_type', 'document_text', 'cleaned_text', 'processed_text']].to_json(\n",
    "    output_json_path, orient='records', indent=2\n",
    ")\n",
    "print(f'Full cleaned data saved to: {output_json_path}')\n",
    "\n",
    "# Verify saved CSV\n",
    "print('\\n=== CSV Verification ===')\n",
    "verify_csv = pd.read_csv(output_csv_path)\n",
    "print(f'Reloaded CSV shape: {verify_csv.shape}')\n",
    "print(f'Columns: {list(verify_csv.columns)}')\n",
    "sample_csv = verify_csv['processed_text'].iloc[0][:200]\n",
    "print(f'Sample processed_text (list format): {sample_csv}')\n",
    "\n",
    "# Verify saved JSON loads correctly\n",
    "print('\\n=== JSON Verification ===')\n",
    "verify_json = pd.read_json(output_json_path, orient='records')\n",
    "print(f'Reloaded JSON shape: {verify_json.shape}')\n",
    "print(f'Columns: {list(verify_json.columns)}')\n",
    "sample_json = verify_json['processed_text'].iloc[0]\n",
    "print(f'processed_text type in JSON: {type(sample_json)}')\n",
    "print(f'Sample processed_text (first 10 tokens): {sample_json[:10]}')\n",
    "\n",
    "print('\\nFirst few rows of cleaned data:')\n",
    "verify_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\nExport your completed work as HTML. Select **File** > **Download as** > **HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}