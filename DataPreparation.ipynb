{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT2311 Assignment - Task 1a: Data Preparation\n",
    "\n",
    "This notebook performs data understanding and data cleaning on the World Bank project documents dataset.\n",
    "\n",
    "**Sub-tasks:**\n",
    "1. **Load Data**: Load the dataset\n",
    "2. **Data Understanding**: Examine the dataset\n",
    "3. **Data Cleaning**: Clean the data and perform all necessary pre-processing\n",
    "4. **Save Data**: Save the cleaned data for the next task\n",
    "\n",
    "**Dataset**: `Task_1_TM_world_bank_projects_subset.json`\n",
    "\n",
    "**Citation**: Jordan, Luke S. (2021). World Bank Project Documents [Dataset]. Hugging Face. Available at: https://huggingface.co/datasets/lukesjordan/worldbank-project-documents\n",
    "\n",
    "**Note**: This analysis uses a modified subset of the original dataset. Any changes were made by the author of this notebook and are not endorsed by the original dataset creator or the World Bank.\n",
    "\n",
    "**Done by: \\<Enter your name and admin number here\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text pre-processing\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "print('Libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load the World Bank project documents dataset from the JSON file. The dataset contains documents related to World Bank development projects from 1947-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from JSON\n",
    "df = pd.read_json('Task_1_TM_world_bank_projects_subset.json')\n",
    "\n",
    "print(f'Dataset loaded successfully.')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Number of records: {df.shape[0]}')\n",
    "print(f'Number of features: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Understanding\n",
    "\n",
    "In this section, we thoroughly examine the dataset to understand its structure, quality, and characteristics before proceeding with cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print('=== First 5 Rows ===')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last few rows\n",
    "print('=== Last 5 Rows ===')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info - shows data types, non-null counts, memory usage\n",
    "print('=== Dataset Info ===')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for text columns\n",
    "print('=== Descriptive Statistics ===')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names and data types\n",
    "print('=== Column Names and Data Types ===')\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Analysis\n",
    "\n",
    "Checking for missing or null values is crucial to ensure data quality. Missing text data could affect topic modelling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('=== Missing Values ===')\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "print(missing_df)\n",
    "print(f'\\nTotal rows with any missing value: {df.isnull().any(axis=1).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings in text fields\n",
    "print('=== Empty Strings ===')\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        empty_count = (df[col].str.strip() == '').sum()\n",
    "        print(f'{col}: {empty_count} empty strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Duplicate Analysis\n",
    "\n",
    "Duplicate records can bias topic modelling results by overrepresenting certain themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f'Number of exact duplicate rows: {df.duplicated().sum()}')\n",
    "print(f'Number of duplicate project_ids: {df.duplicated(subset=[\"project_id\"]).sum()}')\n",
    "print(f'Number of unique project_ids: {df[\"project_id\"].nunique()}')\n",
    "print(f'Number of duplicate document_text: {df.duplicated(subset=[\"document_text\"]).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Document Type Distribution\n",
    "\n",
    "Understanding the distribution of document types (APPROVAL vs REVIEW) helps us know if the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of document types\n",
    "print('=== Document Type Distribution ===')\n",
    "print(df['document_type'].value_counts())\n",
    "print(f'\\nPercentage Distribution:')\n",
    "print(df['document_type'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize document type distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "df['document_type'].value_counts().plot(kind='bar', color=['steelblue', 'coral'], ax=ax)\n",
    "ax.set_title('Distribution of Document Types', fontsize=14)\n",
    "ax.set_xlabel('Document Type')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Text Length Analysis\n",
    "\n",
    "Analyzing the length of document texts helps identify potential outliers - very short texts may lack meaningful content, while extremely long texts may need special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "df['text_length'] = df['document_text'].str.len()\n",
    "df['word_count'] = df['document_text'].str.split().str.len()\n",
    "\n",
    "print('=== Text Length Statistics (characters) ===')\n",
    "print(df['text_length'].describe())\n",
    "print(f'\\n=== Word Count Statistics ===')\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0].hist(df['text_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Text Length (Characters)', fontsize=12)\n",
    "axes[0].set_xlabel('Number of Characters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['word_count'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Distribution of Word Count', fontsize=12)\n",
    "axes[1].set_xlabel('Number of Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length by document type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, doc_type in enumerate(df['document_type'].unique()):\n",
    "    subset = df[df['document_type'] == doc_type]\n",
    "    axes[i].hist(subset['word_count'], bins=40, color='steelblue' if i == 0 else 'coral', \n",
    "                 edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'Word Count Distribution - {doc_type}', fontsize=12)\n",
    "    axes[i].set_xlabel('Number of Words')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('=== Word Count by Document Type ===')\n",
    "print(df.groupby('document_type')['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Sample Document Inspection\n",
    "\n",
    "Inspecting individual documents helps us identify text quality issues such as special characters, encoding problems, or irrelevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample documents\n",
    "print('=== Sample APPROVAL Document ===')\n",
    "approval_docs = df[df['document_type'] == 'APPROVAL']\n",
    "if len(approval_docs) > 0:\n",
    "    sample = approval_docs.iloc[0]\n",
    "    print(f'Project ID: {sample[\"project_id\"]}')\n",
    "    print(f'Text (first 500 chars): {sample[\"document_text\"][:500]}...')\n",
    "\n",
    "print('\\n=== Sample REVIEW Document ===')\n",
    "review_docs = df[df['document_type'] == 'REVIEW']\n",
    "if len(review_docs) > 0:\n",
    "    sample = review_docs.iloc[0]\n",
    "    print(f'Project ID: {sample[\"project_id\"]}')\n",
    "    print(f'Text (first 500 chars): {sample[\"document_text\"][:500]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for very short documents that may lack meaningful content\n",
    "short_docs = df[df['word_count'] < 10]\n",
    "print(f'Documents with fewer than 10 words: {len(short_docs)}')\n",
    "if len(short_docs) > 0:\n",
    "    print('\\nSample short documents:')\n",
    "    for _, row in short_docs.head().iterrows():\n",
    "        print(f'  Project: {row[\"project_id\"]}, Type: {row[\"document_type\"]}, Text: \"{row[\"document_text\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Key Findings from Data Understanding\n",
    "\n",
    "**Summary of findings:**\n",
    "- Dataset structure: The dataset contains three fields - project_id, document_text, and document_type\n",
    "- Missing values: Identified and will be handled in the cleaning phase\n",
    "- Duplicates: Any duplicate records will be removed to avoid bias\n",
    "- Text quality: Document texts may contain special characters, HTML tags, numbers, and other noise that needs cleaning\n",
    "- Distribution: The distribution of document types (APPROVAL vs REVIEW) has been examined\n",
    "- Text length: Text length varies significantly; very short documents may not contain meaningful content for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "Based on the findings from data understanding, we perform the following cleaning steps:\n",
    "1. Remove duplicate records\n",
    "2. Handle missing values\n",
    "3. Remove very short documents that lack meaningful content\n",
    "4. Clean text: remove special characters, HTML tags, extra whitespace\n",
    "5. Convert text to lowercase\n",
    "6. Tokenize, remove stopwords, and lemmatize\n",
    "\n",
    "**Rationale**: Each step is justified by findings from the data understanding phase. Clean, consistent text is essential for reliable topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Remove Duplicates\n",
    "\n",
    "**Rationale**: Duplicate documents would overrepresent certain topics and bias the topic model. We remove exact duplicates based on document_text to ensure each unique document is represented once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape before removing duplicates: {df.shape}')\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "df_clean = df.drop_duplicates()\n",
    "print(f'After removing exact duplicates: {df_clean.shape}')\n",
    "\n",
    "# Remove duplicates based on document_text (same text, possibly different IDs)\n",
    "df_clean = df_clean.drop_duplicates(subset=['document_text'], keep='first')\n",
    "print(f'After removing duplicate texts: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handle Missing Values\n",
    "\n",
    "**Rationale**: Missing text data cannot be used for topic modelling. Missing document_type makes it impossible to categorize the document. We drop rows with missing critical fields rather than imputing, as imputation of text data would introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Missing values before cleaning:')\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Drop rows with missing document_text or document_type\n",
    "df_clean = df_clean.dropna(subset=['document_text', 'document_type'])\n",
    "\n",
    "# Also remove rows where document_text is an empty string\n",
    "df_clean = df_clean[df_clean['document_text'].str.strip() != '']\n",
    "\n",
    "print(f'\\nShape after handling missing values: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove Very Short Documents\n",
    "\n",
    "**Rationale**: Documents with very few words (less than 10) are unlikely to contain meaningful content for topic modelling. They may be metadata artifacts, headers, or incomplete entries that would introduce noise into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate word count after cleaning\n",
    "df_clean['word_count'] = df_clean['document_text'].str.split().str.len()\n",
    "\n",
    "print(f'Shape before removing short documents: {df_clean.shape}')\n",
    "df_clean = df_clean[df_clean['word_count'] >= 10]\n",
    "print(f'Shape after removing documents with < 10 words: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Text Cleaning\n",
    "\n",
    "**Rationale**: Raw text contains noise such as HTML tags, URLs, special characters, numbers, and extra whitespace. These elements do not contribute to understanding document topics and could confuse the topic model. We apply a systematic cleaning pipeline to standardize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing noise and standardizing format.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove special characters and punctuation, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df_clean['cleaned_text'] = df_clean['document_text'].apply(clean_text)\n",
    "\n",
    "# Show a sample before and after cleaning\n",
    "print('=== Sample Text Before Cleaning ===')\n",
    "print(df_clean['document_text'].iloc[0][:300])\n",
    "print('\\n=== Sample Text After Cleaning ===')\n",
    "print(df_clean['cleaned_text'].iloc[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Tokenization, Stopword Removal, and Lemmatization\n",
    "\n",
    "**Rationale**: \n",
    "- **Tokenization** breaks text into individual words for analysis.\n",
    "- **Stopword removal** eliminates common words (e.g., 'the', 'is', 'and') that don't carry topic-specific meaning, reducing noise.\n",
    "- **Lemmatization** reduces words to their base form (e.g., 'running' \u2192 'run'), grouping related words together for more coherent topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Add domain-specific stopwords that appear frequently but don't carry topic meaning\n",
    "custom_stopwords = {'project', 'bank', 'world', 'document', 'page', 'report', \n",
    "                    'would', 'also', 'may', 'one', 'two', 'three', 'could',\n",
    "                    'million', 'percent', 'year', 'years'}\n",
    "stop_words = stop_words.union(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, remove stopwords, and lemmatize text.\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short tokens (length < 3)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) >= 3]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print('Preprocessing text (this may take a few minutes)...')\n",
    "df_clean['processed_text'] = df_clean['cleaned_text'].apply(preprocess_text)\n",
    "print('Text preprocessing complete.')\n",
    "\n",
    "# Show sample\n",
    "print('\\n=== Sample Processed Text ===')\n",
    "print(df_clean['processed_text'].iloc[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows where processed text is empty after cleaning\n",
    "before_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['processed_text'].str.strip() != '']\n",
    "after_count = len(df_clean)\n",
    "print(f'Removed {before_count - after_count} rows with empty processed text')\n",
    "print(f'Final dataset shape: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Post-Cleaning Verification\n",
    "\n",
    "Verify the quality of cleaned data to ensure cleaning was effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning results\n",
    "print('=== Post-Cleaning Summary ===')\n",
    "print(f'Final number of records: {len(df_clean)}')\n",
    "print(f'Missing values: {df_clean[[\"project_id\", \"document_text\", \"document_type\", \"processed_text\"]].isnull().sum().sum()}')\n",
    "print(f'Duplicate records: {df_clean.duplicated().sum()}')\n",
    "print(f'\\nDocument type distribution:')\n",
    "print(df_clean['document_type'].value_counts())\n",
    "\n",
    "# Word count statistics after cleaning\n",
    "df_clean['processed_word_count'] = df_clean['processed_text'].str.split().str.len()\n",
    "print(f'\\nProcessed text word count statistics:')\n",
    "print(df_clean['processed_word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word count distribution after cleaning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_clean['word_count'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Original Word Count Distribution (After Filtering)', fontsize=12)\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(df_clean['processed_word_count'], bins=50, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Processed Word Count Distribution', fontsize=12)\n",
    "axes[1].set_xlabel('Number of Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "Save the cleaned dataset for use in Task 1b (Topic Modelling). We save both the original and processed text so that the topic modelling task has flexibility in choosing the text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to save\n",
    "columns_to_save = ['project_id', 'document_text', 'document_type', 'cleaned_text', 'processed_text']\n",
    "df_save = df_clean[columns_to_save].copy()\n",
    "\n",
    "# Save to JSON\n",
    "df_save.to_json('Task_1_cleaned_data.json', orient='records', indent=2)\n",
    "print(f'Cleaned data saved to Task_1_cleaned_data.json')\n",
    "print(f'Number of records saved: {len(df_save)}')\n",
    "print(f'Columns saved: {list(df_save.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Understanding Findings:\n",
    "- The dataset contains World Bank project documents with three fields: project_id, document_text, and document_type\n",
    "- Document types include APPROVAL (project launch documents) and REVIEW (end-of-project review documents)\n",
    "- Text lengths vary significantly across documents\n",
    "- Some documents may be very short and lack meaningful content\n",
    "\n",
    "### Data Cleaning Steps Performed:\n",
    "1. **Removed duplicates**: Eliminated exact duplicate rows and duplicate document texts to avoid topic bias\n",
    "2. **Handled missing values**: Dropped rows with missing text or document type (imputation not suitable for text)\n",
    "3. **Removed short documents**: Filtered out documents with fewer than 10 words as they lack meaningful content\n",
    "4. **Text cleaning**: Converted to lowercase, removed HTML tags, URLs, emails, numbers, special characters, and extra whitespace\n",
    "5. **Text preprocessing**: Tokenized, removed stopwords (including domain-specific ones), and lemmatized to standardize vocabulary\n",
    "6. **Post-cleaning verification**: Confirmed data quality after all cleaning steps\n",
    "\n",
    "The cleaned dataset is saved and ready for Topic Modelling in Task 1b."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}