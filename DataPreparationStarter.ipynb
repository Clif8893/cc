{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYP IT2311 Assignment - Task 1a: Data Preparation\n",
    "\n",
    "**Done by:** [Your Name] [Your Admin Number]\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **data preparation** on the World Bank Project Documents dataset. The goal is to load, explore, understand, and clean the data so it is ready for downstream NLP tasks such as sentiment classification or text analysis.\n",
    "\n",
    "Data preparation is a critical first step in any data science pipeline. Poorly prepared data leads to unreliable models and misleading conclusions. In this notebook, we take a methodical approach:\n",
    "\n",
    "1. **Load** the raw JSON dataset\n",
    "2. **Understand** the structure, distributions, and quality of the data through exploration and visualization\n",
    "3. **Clean** the data by handling missing values, duplicates, and noisy text\n",
    "4. **Save** the cleaned dataset for use in subsequent tasks\n",
    "\n",
    "Each step includes detailed rationale explaining *why* specific decisions were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries\n",
    "\n",
    "We begin by importing the necessary Python libraries:\n",
    "\n",
    "- **pandas** and **numpy**: Core data manipulation and numerical computing libraries. Pandas provides DataFrame structures ideal for tabular data, while NumPy supports efficient numerical operations.\n",
    "- **matplotlib** and **seaborn**: Visualization libraries. Seaborn builds on matplotlib and provides a higher-level interface for statistical graphics, making it easier to produce informative plots.\n",
    "- **json**: For handling JSON file operations beyond what pandas provides natively.\n",
    "- **re**: Python's regular expression module, essential for text cleaning operations such as removing HTML tags, URLs, and special characters.\n",
    "- **warnings**: We suppress warnings to keep the notebook output clean and focused on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visual style for all plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('All libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data\n",
    "\n",
    "The dataset is stored in JSON format (`Task_1_TM_world_bank_projects_subset.json`). JSON (JavaScript Object Notation) is a lightweight, human-readable data interchange format commonly used for web APIs and document storage.\n",
    "\n",
    "We use `pd.read_json()` to load it directly into a pandas DataFrame. This is the most straightforward approach for structured JSON files. If the JSON structure were deeply nested, we might need `json.load()` followed by `pd.json_normalize()`, but for this dataset the direct approach is appropriate.\n",
    "\n",
    "After loading, we immediately inspect the data to confirm it loaded correctly and to get an initial sense of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON dataset\n",
    "df = pd.read_json('Task_1_TM_world_bank_projects_subset.json')\n",
    "\n",
    "print(f'Dataset loaded successfully.')\n",
    "print(f'Shape: {df.shape[0]} rows x {df.shape[1]} columns')\n",
    "print(f'Columns: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to get a visual overview of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed information about the DataFrame\n",
    "# This tells us column names, non-null counts, and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The dataset has been loaded into a DataFrame. We can see three columns as expected: `project_id`, `document_text`, and `document_type`. The `.info()` output reveals the data types and whether there are any null values at a glance. This initial inspection is crucial \u2014 it tells us whether the data loaded correctly and gives us a roadmap for the exploration ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Understanding\n",
    "\n",
    "Before any cleaning or transformation, it is essential to deeply understand the data. This phase answers key questions:\n",
    "\n",
    "- **What does the data look like?** (types, shape, distributions)\n",
    "- **What quality issues exist?** (missing values, duplicates, anomalies)\n",
    "- **What patterns or insights can we discover?** (class balance, text characteristics)\n",
    "\n",
    "A thorough understanding phase prevents us from making uninformed decisions during cleaning and ensures we preserve important information while removing genuine noise. It also helps us anticipate challenges for downstream tasks.\n",
    "\n",
    "### 2.1 Basic Statistics and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types for each column\n",
    "print('=== Data Types ===')\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "# Descriptive statistics for all columns\n",
    "print('=== Descriptive Statistics ===')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** The `describe(include='all')` output gives us summary statistics for all columns regardless of type. For text columns, it shows count, unique values, top (most frequent) value, and frequency. This is our first quantitative look at the data's characteristics. Notably, we can see how many unique project IDs and document types exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values Analysis\n",
    "\n",
    "Missing values are one of the most common data quality issues. They can arise from data collection errors, system failures, or intentional omissions. Understanding the pattern of missingness is crucial because:\n",
    "\n",
    "- **Missing Completely at Random (MCAR):** Safe to drop or impute without bias\n",
    "- **Missing at Random (MAR):** Missingness depends on observed variables\n",
    "- **Missing Not at Random (MNAR):** Missingness depends on the missing value itself\n",
    "\n",
    "We visualize missing values using a heatmap, which makes it easy to spot patterns of missingness across columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage (%)': missing_pct.round(2)\n",
    "})\n",
    "print('=== Missing Values Summary ===')\n",
    "print(missing_df)\n",
    "print(f'\\nTotal missing values: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values with a heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis', ax=ax)\n",
    "ax.set_title('Missing Values Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Columns')\n",
    "ax.set_ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Yellow regions indicate missing values. A completely dark heatmap means no missing data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The heatmap provides a visual summary of data completeness. If we see yellow streaks or blocks, that would indicate systematic missing data patterns worth investigating further. A uniformly dark heatmap is the ideal scenario, indicating no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Duplicate Records Analysis\n",
    "\n",
    "Duplicates can inflate our dataset artificially and bias any model trained on it. In NLP tasks, duplicate texts are especially problematic because they can cause data leakage between training and test sets. We check for:\n",
    "\n",
    "1. **Exact duplicates** \u2014 rows where every column matches\n",
    "2. **Duplicate document texts** \u2014 same text appearing with different project IDs or types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for exact duplicate rows\n",
    "exact_dupes = df.duplicated().sum()\n",
    "print(f'Exact duplicate rows: {exact_dupes}')\n",
    "print(f'Percentage of duplicates: {(exact_dupes / len(df) * 100):.2f}%')\n",
    "\n",
    "# Check for duplicate document_text values\n",
    "text_dupes = df['document_text'].duplicated().sum()\n",
    "print(f'\\nDuplicate document_text values: {text_dupes}')\n",
    "\n",
    "# Check for duplicate project_id values\n",
    "id_dupes = df['project_id'].duplicated().sum()\n",
    "print(f'Duplicate project_id values: {id_dupes}')\n",
    "\n",
    "if exact_dupes > 0:\n",
    "    print(f'\\nSample duplicate rows:')\n",
    "    display(df[df.duplicated(keep=False)].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:** Understanding the nature of duplicates is important. A project may legitimately have multiple documents (e.g., both an APPROVAL and a REVIEW document), so duplicate `project_id` values are expected. However, exact duplicate rows (identical across all columns) are likely data entry errors and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Document Type Distribution\n",
    "\n",
    "Since `document_type` is our categorical variable (with values \"APPROVAL\" and \"REVIEW\"), understanding its distribution is critical. Class imbalance can significantly affect downstream classification tasks \u2014 a model trained on an imbalanced dataset may develop a bias toward the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values and their counts\n",
    "print('=== Document Type Value Counts ===')\n",
    "type_counts = df['document_type'].value_counts()\n",
    "print(type_counts)\n",
    "print(f'\\nNumber of unique document types: {df[\"document_type\"].nunique()}')\n",
    "print(f'\\nProportions:')\n",
    "print(df['document_type'].value_counts(normalize=True).round(4) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart showing document type distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "type_counts.plot.pie(\n",
    "    ax=axes[0],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    explode=[0.03] * len(type_counts),\n",
    "    shadow=True\n",
    ")\n",
    "axes[0].set_title('Document Type Distribution (Pie)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Bar chart\n",
    "type_counts.plot.bar(ax=axes[1], color=colors, edgecolor='black')\n",
    "axes[1].set_title('Document Type Distribution (Bar)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Document Type')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(type_counts.index, rotation=0)\n",
    "\n",
    "# Annotate bars with counts\n",
    "for i, (val, name) in enumerate(zip(type_counts.values, type_counts.index)):\n",
    "    axes[1].text(i, val + 0.5, str(val), ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discovery:** The pie and bar charts reveal the balance (or imbalance) between APPROVAL and REVIEW documents. If one class significantly outnumbers the other, we may need to consider techniques like oversampling, undersampling, or class weights during modeling. Even in data preparation, this awareness guides our cleaning decisions \u2014 we should be cautious not to disproportionately remove records from the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Text Length and Word Count Analysis\n",
    "\n",
    "Analyzing the length distribution of `document_text` provides valuable insights:\n",
    "\n",
    "- **Very short texts** may lack meaningful content (e.g., headers or placeholders)\n",
    "- **Very long texts** may include boilerplate or repeated sections\n",
    "- **Distribution shape** tells us whether most documents are similar in length or if there's wide variation\n",
    "\n",
    "We examine both character length and word count, as they capture different aspects of text complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text length (characters) and word count for each document\n",
    "df['text_length'] = df['document_text'].astype(str).apply(len)\n",
    "df['word_count'] = df['document_text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print('=== Text Length Statistics (Characters) ===')\n",
    "print(df['text_length'].describe().round(2))\n",
    "print()\n",
    "print('=== Word Count Statistics ===')\n",
    "print(df['word_count'].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length and word count distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0].hist(df['text_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', label=f\"Mean: {df['text_length'].mean():.0f}\")\n",
    "axes[0].axvline(df['text_length'].median(), color='orange', linestyle='--', label=f\"Median: {df['text_length'].median():.0f}\")\n",
    "axes[0].set_title('Distribution of Text Length (Characters)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['word_count'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['word_count'].mean(), color='red', linestyle='--', label=f\"Mean: {df['word_count'].mean():.0f}\")\n",
    "axes[1].axvline(df['word_count'].median(), color='orange', linestyle='--', label=f\"Median: {df['word_count'].median():.0f}\")\n",
    "axes[1].set_title('Distribution of Word Count', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The histograms reveal whether the text lengths follow a normal, skewed, or multimodal distribution. A right-skewed distribution (long tail to the right) would indicate that most documents are relatively short, with a few very long outliers. The mean and median lines help us assess skewness \u2014 a large gap between them indicates significant skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Text Length Comparison: APPROVAL vs REVIEW\n",
    "\n",
    "A key question is whether APPROVAL and REVIEW documents have systematically different text characteristics. If one type tends to be longer or shorter, this could be an informative feature for classification. It also affects our cleaning thresholds \u2014 we need to ensure our cleaning doesn't inadvertently bias toward one document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text length by document type\n",
    "print('=== Text Length by Document Type ===')\n",
    "print(df.groupby('document_type')['text_length'].describe().round(2))\n",
    "print()\n",
    "print('=== Word Count by Document Type ===')\n",
    "print(df.groupby('document_type')['word_count'].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot comparing text length across document types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Text length boxplot\n",
    "sns.boxplot(x='document_type', y='text_length', data=df, ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Text Length by Document Type', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Document Type')\n",
    "axes[0].set_ylabel('Text Length (characters)')\n",
    "\n",
    "# Word count boxplot\n",
    "sns.boxplot(x='document_type', y='word_count', data=df, ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Word Count by Document Type', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Document Type')\n",
    "axes[1].set_ylabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discovery:** The boxplots allow us to compare the central tendency, spread, and outliers of text lengths between document types. Key observations to look for:\n",
    "- **Median differences:** Do APPROVAL and REVIEW documents tend to differ in length?\n",
    "- **Spread (IQR):** Is one type more variable in length than the other?\n",
    "- **Outliers:** Are there extreme values that might need special handling?\n",
    "\n",
    "These differences, if they exist, could serve as useful features for classification and also inform our cleaning thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Check for Empty or Whitespace-Only Texts\n",
    "\n",
    "Empty or whitespace-only texts provide no value for analysis and should be identified. These can arise from data extraction errors where the document metadata was captured but the actual text content was not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings\n",
    "empty_texts = df[df['document_text'].astype(str).str.strip() == ''].shape[0]\n",
    "print(f'Empty or whitespace-only texts: {empty_texts}')\n",
    "\n",
    "# Check for very short texts (less than 50 characters)\n",
    "very_short = df[df['text_length'] < 50].shape[0]\n",
    "print(f'Very short texts (< 50 chars): {very_short}')\n",
    "\n",
    "# Check for NaN converted to string \"nan\"\n",
    "nan_strings = df[df['document_text'].astype(str).str.lower() == 'nan'].shape[0]\n",
    "print(f'Texts that are literal \"nan\": {nan_strings}')\n",
    "\n",
    "if very_short > 0:\n",
    "    print(f'\\n--- Sample very short texts ---')\n",
    "    display(df[df['text_length'] < 50][['project_id', 'document_type', 'document_text', 'text_length']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Sample Texts from Each Document Type\n",
    "\n",
    "Reading actual sample texts gives us qualitative insight that statistics alone cannot provide. By examining real examples, we can identify:\n",
    "- Common vocabulary and phrasing patterns\n",
    "- Presence of HTML tags, URLs, or other artifacts\n",
    "- Structural differences between document types\n",
    "- Potential noise that needs to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample texts from each document type\n",
    "for doc_type in df['document_type'].unique():\n",
    "    print(f'\\n{\"=\" * 80}')\n",
    "    print(f'Sample {doc_type} document:')\n",
    "    print(f'{\"=\" * 80}')\n",
    "    sample_text = df[df['document_type'] == doc_type]['document_text'].iloc[0]\n",
    "    # Show first 500 characters to keep output manageable\n",
    "    print(str(sample_text)[:500])\n",
    "    print('...' if len(str(sample_text)) > 500 else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection on Data Understanding Phase:**\n",
    "\n",
    "Through this comprehensive exploration, we have built a solid understanding of our dataset:\n",
    "- We know the data types, shape, and structure\n",
    "- We have identified the presence (or absence) of missing values and duplicates\n",
    "- We understand the class distribution of document types\n",
    "- We have characterized the text length distributions and compared them across document types\n",
    "- We have examined actual text samples to identify potential noise and artifacts\n",
    "\n",
    "This understanding now informs our cleaning strategy in the next section. Every cleaning decision will be grounded in what we discovered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning\n",
    "\n",
    "Data cleaning transforms raw, noisy data into a form suitable for analysis. Our cleaning strategy is guided by the findings from Section 2. The key principle is: **clean aggressively enough to remove genuine noise, but conservatively enough to preserve meaningful information.**\n",
    "\n",
    "Our cleaning pipeline consists of:\n",
    "1. Handle missing values\n",
    "2. Remove duplicate records\n",
    "3. Clean text content (HTML, URLs, special characters, whitespace)\n",
    "4. Remove very short documents\n",
    "5. Validate results\n",
    "\n",
    "### 3.1 Handle Missing Values\n",
    "\n",
    "Missing values need to be addressed first because they can cause errors in subsequent cleaning steps. Our strategy depends on the column:\n",
    "- **document_text:** Rows with missing text have no value for NLP tasks \u2014 we drop them\n",
    "- **document_type:** This is our label; rows without it cannot be used for classification \u2014 we drop them\n",
    "- **project_id:** Missing IDs are less critical but still dropped for data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original shape for comparison\n",
    "original_shape = df.shape\n",
    "print(f'Original dataset shape: {original_shape}')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna(subset=['document_text', 'document_type', 'project_id']).copy()\n",
    "\n",
    "rows_dropped_missing = original_shape[0] - df_cleaned.shape[0]\n",
    "print(f'Rows dropped due to missing values: {rows_dropped_missing}')\n",
    "print(f'Shape after handling missing values: {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale:** We chose to drop rows with missing values rather than impute them because:\n",
    "- Text data cannot be meaningfully imputed \u2014 generating synthetic document text would introduce artificial data\n",
    "- Document type labels must be accurate for classification; guessing would introduce noise\n",
    "- The cost of dropping a few rows is minimal compared to the risk of introducing bad data\n",
    "\n",
    "### 3.2 Remove Duplicate Records\n",
    "\n",
    "Duplicate records artificially inflate dataset size and can cause data leakage in train/test splits. We remove exact duplicates (all columns identical) to ensure each record is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exact duplicate rows\n",
    "before_dedup = df_cleaned.shape[0]\n",
    "df_cleaned = df_cleaned.drop_duplicates().reset_index(drop=True)\n",
    "after_dedup = df_cleaned.shape[0]\n",
    "\n",
    "rows_dropped_dupes = before_dedup - after_dedup\n",
    "print(f'Rows dropped due to duplicates: {rows_dropped_dupes}')\n",
    "print(f'Shape after deduplication: {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale:** We use `drop_duplicates()` which considers all columns. This is the safest approach because it only removes rows that are identical in every aspect. We keep the first occurrence by default, which is a standard practice. The `reset_index(drop=True)` ensures a clean, sequential index after removal.\n",
    "\n",
    "### 3.3 Clean Document Text\n",
    "\n",
    "Text cleaning is the most critical step for NLP tasks. Raw text often contains artifacts that add noise without contributing meaning. Our cleaning pipeline addresses each type of noise systematically:\n",
    "\n",
    "1. **HTML tags** (e.g., `<p>`, `<br>`, `<div>`): These are markup artifacts from web scraping, not part of the actual document content\n",
    "2. **URLs** (e.g., `http://...`, `www...`): Web addresses don't contribute to document meaning in this context\n",
    "3. **Special characters**: Non-alphanumeric characters (except basic punctuation) can interfere with tokenization\n",
    "4. **Case normalization**: Converting to lowercase ensures \"Project\" and \"project\" are treated as the same word\n",
    "5. **Extra whitespace**: Multiple spaces, tabs, and newlines are collapsed for consistency\n",
    "\n",
    "Each transformation is applied sequentially, and we preserve the order to avoid conflicts (e.g., removing HTML before special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning pipeline.\n",
    "    Applies a sequence of regex-based transformations to remove noise from text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Step 1: Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    # Step 3: Remove special characters (keep letters, numbers, basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\\\"()-]', ' ', text)\n",
    "    \n",
    "    # Step 4: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 5: Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print('Text cleaning function defined successfully.')\n",
    "print('Pipeline: HTML removal -> URL removal -> Special char removal -> Lowercase -> Whitespace normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original text for before/after comparison\n",
    "df_cleaned['original_text'] = df_cleaned['document_text'].copy()\n",
    "\n",
    "# Apply the cleaning function to all document texts\n",
    "df_cleaned['document_text'] = df_cleaned['document_text'].astype(str).apply(clean_text)\n",
    "\n",
    "print('Text cleaning applied to all documents.')\n",
    "print(f'Dataset shape: {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Before/After Comparison\n",
    "\n",
    "It is essential to visually verify that our cleaning pipeline is working as intended. By comparing original and cleaned text side by side, we can confirm that:\n",
    "- Noise has been removed\n",
    "- Meaningful content has been preserved\n",
    "- No unintended modifications occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show before/after comparison for sample texts\n",
    "print('=== Before vs After Cleaning Comparison ===')\n",
    "for i in range(min(3, len(df_cleaned))):\n",
    "    print(f'\\n--- Document {i + 1} (Type: {df_cleaned.iloc[i][\"document_type\"]}) ---')\n",
    "    print(f'BEFORE (first 300 chars):')\n",
    "    print(str(df_cleaned.iloc[i]['original_text'])[:300])\n",
    "    print(f'\\nAFTER (first 300 chars):')\n",
    "    print(str(df_cleaned.iloc[i]['document_text'])[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Remove Very Short Documents\n",
    "\n",
    "Documents that are extremely short after cleaning likely contain no meaningful content for analysis. These could be:\n",
    "- Documents that were mostly HTML/URLs and had little actual text\n",
    "- Placeholder or error documents\n",
    "- Metadata fragments\n",
    "\n",
    "We set a minimum threshold of **50 characters** after cleaning. This threshold is chosen because:\n",
    "- 50 characters is approximately 8-10 words, the minimum for a meaningful sentence\n",
    "- It removes genuinely empty/useless records without being overly aggressive\n",
    "- It preserves the vast majority of legitimate documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate text length after cleaning\n",
    "df_cleaned['cleaned_text_length'] = df_cleaned['document_text'].apply(len)\n",
    "df_cleaned['cleaned_word_count'] = df_cleaned['document_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Identify very short documents\n",
    "min_length_threshold = 50\n",
    "short_docs = df_cleaned[df_cleaned['cleaned_text_length'] < min_length_threshold]\n",
    "print(f'Documents shorter than {min_length_threshold} characters after cleaning: {len(short_docs)}')\n",
    "\n",
    "if len(short_docs) > 0:\n",
    "    print(f'\\nShort document type distribution:')\n",
    "    print(short_docs['document_type'].value_counts())\n",
    "    print(f'\\nSample short documents:')\n",
    "    display(short_docs[['project_id', 'document_type', 'document_text', 'cleaned_text_length']].head(5))\n",
    "\n",
    "# Remove very short documents\n",
    "before_short_removal = df_cleaned.shape[0]\n",
    "df_cleaned = df_cleaned[df_cleaned['cleaned_text_length'] >= min_length_threshold].reset_index(drop=True)\n",
    "after_short_removal = df_cleaned.shape[0]\n",
    "\n",
    "print(f'\\nRows dropped (too short): {before_short_removal - after_short_removal}')\n",
    "print(f'Shape after removing short documents: {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale for 50-character threshold:** We chose 50 characters as the minimum because it strikes a balance between removing genuinely meaningless records and preserving data. A shorter threshold (e.g., 10 characters) would leave noise in the dataset, while a longer one (e.g., 200 characters) might remove legitimate short but meaningful documents. The threshold was informed by our exploration in Section 2.5 where we examined the text length distribution.\n",
    "\n",
    "### 3.6 Validate Cleaning Results\n",
    "\n",
    "After all cleaning steps, we perform a comprehensive validation to ensure data quality. This includes checking that:\n",
    "- No missing values remain\n",
    "- No duplicates remain\n",
    "- Text lengths are reasonable\n",
    "- Class distribution hasn't been severely distorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Post-Cleaning Validation ===')\n",
    "print(f'Final dataset shape: {df_cleaned.shape}')\n",
    "print(f'\\nMissing values:')\n",
    "print(df_cleaned[['project_id', 'document_text', 'document_type']].isnull().sum())\n",
    "print(f'\\nDuplicate rows: {df_cleaned.duplicated().sum()}')\n",
    "print(f'\\nDocument type distribution after cleaning:')\n",
    "print(df_cleaned['document_type'].value_counts())\n",
    "print(f'\\nText length statistics after cleaning:')\n",
    "print(df_cleaned['cleaned_text_length'].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution changes after cleaning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cleaned text length distribution\n",
    "axes[0].hist(df_cleaned['cleaned_text_length'], bins=50, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df_cleaned['cleaned_text_length'].mean(), color='red', linestyle='--',\n",
    "                label=f\"Mean: {df_cleaned['cleaned_text_length'].mean():.0f}\")\n",
    "axes[0].axvline(df_cleaned['cleaned_text_length'].median(), color='orange', linestyle='--',\n",
    "                label=f\"Median: {df_cleaned['cleaned_text_length'].median():.0f}\")\n",
    "axes[0].set_title('Text Length Distribution (After Cleaning)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cleaned word count distribution\n",
    "axes[1].hist(df_cleaned['cleaned_word_count'], bins=50, color='mediumpurple', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df_cleaned['cleaned_word_count'].mean(), color='red', linestyle='--',\n",
    "                label=f\"Mean: {df_cleaned['cleaned_word_count'].mean():.0f}\")\n",
    "axes[1].axvline(df_cleaned['cleaned_word_count'].median(), color='orange', linestyle='--',\n",
    "                label=f\"Median: {df_cleaned['cleaned_word_count'].median():.0f}\")\n",
    "axes[1].set_title('Word Count Distribution (After Cleaning)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all cleaning operations\n",
    "print('=' * 60)\n",
    "print('DATA CLEANING SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Original records:          {original_shape[0]}')\n",
    "print(f'Dropped (missing values):  {rows_dropped_missing}')\n",
    "print(f'Dropped (duplicates):      {rows_dropped_dupes}')\n",
    "print(f'Dropped (too short):       {before_short_removal - after_short_removal}')\n",
    "print(f'Final records:             {df_cleaned.shape[0]}')\n",
    "print(f'Total records removed:     {original_shape[0] - df_cleaned.shape[0]}')\n",
    "print(f'Retention rate:            {(df_cleaned.shape[0] / original_shape[0] * 100):.1f}%')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection on Data Cleaning:**\n",
    "\n",
    "Our cleaning pipeline was designed to be thorough yet conservative. The key decisions and their rationale:\n",
    "\n",
    "1. **Dropping missing values** rather than imputing \u2014 text data cannot be meaningfully imputed\n",
    "2. **Removing exact duplicates** \u2014 prevents model bias and data leakage\n",
    "3. **HTML and URL removal** \u2014 these are web artifacts, not meaningful document content\n",
    "4. **Special character removal** \u2014 reduces vocabulary noise for NLP while preserving standard punctuation\n",
    "5. **Lowercase conversion** \u2014 normalizes text to reduce vocabulary size without losing meaning\n",
    "6. **Minimum length threshold** \u2014 removes documents too short to carry meaningful information\n",
    "\n",
    "The cleaning results show that we retained a high percentage of the original data while significantly improving its quality for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Save Cleaned Data\n",
    "\n",
    "We save the cleaned dataset in two formats:\n",
    "- **CSV**: Universal, human-readable, easy to import into any tool\n",
    "- **JSON**: Preserves data types and is ideal for web-based tools and APIs\n",
    "\n",
    "We only save the essential columns (`project_id`, `document_text`, `document_type`) to keep the output files clean and focused. The helper columns (`text_length`, `word_count`, `original_text`, etc.) were used for analysis only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the essential columns for saving\n",
    "columns_to_save = ['project_id', 'document_text', 'document_type']\n",
    "df_final = df_cleaned[columns_to_save].copy()\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = 'Task_1_cleaned_data.csv'\n",
    "df_final.to_csv(csv_path, index=False)\n",
    "print(f'Cleaned data saved to CSV: {csv_path}')\n",
    "\n",
    "# Save to JSON\n",
    "json_path = 'Task_1_cleaned_data.json'\n",
    "df_final.to_json(json_path, orient='records', indent=2)\n",
    "print(f'Cleaned data saved to JSON: {json_path}')\n",
    "\n",
    "# Confirm save\n",
    "print(f'\\nFinal dataset shape: {df_final.shape}')\n",
    "print(f'Columns saved: {list(df_final.columns)}')\n",
    "print(f'\\nFirst 3 rows of saved data:')\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification:** The cleaned data has been saved successfully. The CSV format ensures compatibility with a wide range of tools (Excel, R, SQL), while the JSON format preserves the original data structure. Both files contain only the essential columns and can be loaded directly for Task 1b and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Citation\n",
    "\n",
    "The dataset used in this notebook is sourced from:\n",
    "\n",
    "> Jordan, Luke S. (2021). *World Bank Project Documents* [Dataset]. Hugging Face. \n",
    "\n",
    "This dataset contains a subset of World Bank project documents categorized by document type (APPROVAL and REVIEW), and is used here for educational purposes as part of the NYP IT2311 assignment.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Task 1a: Data Preparation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}