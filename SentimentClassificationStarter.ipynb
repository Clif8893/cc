{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanyang Polytechnic (NYP)\n",
    "\n",
    "# IT2311 Assignment - Task 2: Sentiment Classification\n",
    "\n",
    "We are required to build a sentiment classification model to predict the sentiment of video game review text. Businesses will be able to use this model to predict the sentiment of a new review.\n",
    "\n",
    "Complete the following sub-tasks:\n",
    "1. **Load Data**: Load the dataset and perform initial exploration\n",
    "2. **Data Preparation**: Prepare text representations and engineer features for classification\n",
    "3. **Modelling**: Build sentiment classifiers using different algorithms and text representations\n",
    "4. **Evaluation**: Evaluate results, compare models, and select the best model with justification\n",
    "\n",
    "For each sub-task, the rationale for every step is explained in detail.\n",
    "\n",
    "**Done by: \\<Enter your name and admin number here\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Download Packages\n",
    "\n",
    "We begin by importing all necessary libraries:\n",
    "- **pandas / numpy**: Data manipulation and numerical operations\n",
    "- **matplotlib / seaborn**: Data visualization\n",
    "- **nltk**: Natural Language Processing (tokenization, stopwords, lemmatization)\n",
    "- **sklearn**: Machine learning models, evaluation metrics, vectorizers, and pipelines\n",
    "- **re / warnings**: Text cleaning utilities and warning suppression\n",
    "\n",
    "These libraries form the standard toolkit for NLP-based text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             accuracy_score, f1_score, precision_score, recall_score,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('All libraries imported successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print('NLTK data downloaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data\n",
    "\n",
    "We load the Amazon Video Game Reviews dataset (`Task_2_SA_video_game_reviews.json`) which contains 50,000 product reviews. Each review includes a numeric rating (1.0–5.0), review title, full review text, and metadata such as product ID, user ID, timestamp, helpful votes, and purchase verification.\n",
    "\n",
    "**Rationale**: Understanding the raw data structure is the critical first step. We examine the shape, data types, missing values, and the distribution of the target variable (rating) to inform our preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON dataset\n",
    "df = pd.read_json('Task_2_SA_video_game_reviews.json', lines=True)\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Number of rows: {df.shape[0]:,}')\n",
    "print(f'Number of columns: {df.shape[1]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "print('Missing values per column:')\n",
    "missing_df[missing_df['Missing Count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rating Distribution\n",
    "\n",
    "We visualise the distribution of ratings to understand the class balance. This is crucial because:\n",
    "- Imbalanced classes can bias models toward the majority class\n",
    "- It helps us decide how to map ratings to sentiment categories\n",
    "- It informs whether we need stratified sampling or class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise rating distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of rating counts\n",
    "rating_counts = df['rating'].value_counts().sort_index()\n",
    "axes[0].bar(rating_counts.index.astype(str), rating_counts.values, color=sns.color_palette('viridis', 5))\n",
    "axes[0].set_title('Distribution of Ratings', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(rating_counts.values):\n",
    "    axes[0].text(i, v + 200, f'{v:,}', ha='center', fontsize=10)\n",
    "\n",
    "# Percentage pie chart\n",
    "axes[1].pie(rating_counts.values, labels=[f'{r} Star' for r in rating_counts.index.astype(int)],\n",
    "            autopct='%1.1f%%', colors=sns.color_palette('viridis', 5), startangle=90)\n",
    "axes[1].set_title('Rating Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nRating value counts:')\n",
    "print(rating_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The ratings are likely skewed toward higher values (4–5 stars), which is common in product reviews. This imbalance will need to be addressed during our sentiment mapping and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "\n",
    "Data preparation is the most critical step in any NLP pipeline. We will:\n",
    "1. Create sentiment labels from ratings\n",
    "2. Clean and preprocess the text\n",
    "3. Engineer features using text vectorization\n",
    "4. Split the data for training and testing\n",
    "\n",
    "Each decision is justified with clear rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sentiment Label Creation\n",
    "\n",
    "We map the 5-point rating scale to three sentiment categories:\n",
    "\n",
    "| Rating | Sentiment |\n",
    "|--------|-----------|\n",
    "| 1.0 – 2.0 | **Negative** |\n",
    "| 3.0 | **Neutral** |\n",
    "| 4.0 – 5.0 | **Positive** |\n",
    "\n",
    "**Rationale for this mapping**:\n",
    "- **1–2 stars** clearly indicate dissatisfaction — grouping them captures varying degrees of negativity\n",
    "- **3 stars** represents an ambivalent or mixed review — the reviewer is neither satisfied nor dissatisfied\n",
    "- **4–5 stars** indicate satisfaction — grouping them captures both moderate and strong approval\n",
    "- This 3-class mapping is standard in sentiment analysis literature and provides a good balance between granularity and model performance\n",
    "- A binary (positive/negative) mapping would lose the nuance of neutral reviews, which are valuable for business insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ratings to sentiment labels\n",
    "def map_sentiment(rating):\n",
    "    if rating <= 2.0:\n",
    "        return 'Negative'\n",
    "    elif rating == 3.0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(map_sentiment)\n",
    "\n",
    "# Display sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "sentiment_pct = (sentiment_counts / len(df) * 100).round(2)\n",
    "\n",
    "print('Sentiment Distribution:')\n",
    "print('=' * 40)\n",
    "for sent in ['Positive', 'Neutral', 'Negative']:\n",
    "    print(f'{sent:>10}: {sentiment_counts[sent]:>6,} ({sentiment_pct[sent]:>5.1f}%)')\n",
    "print(f'{\"Total\":>10}: {len(df):>6,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise sentiment distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = {'Positive': '#2ecc71', 'Neutral': '#f39c12', 'Negative': '#e74c3c'}\n",
    "order = ['Negative', 'Neutral', 'Positive']\n",
    "bars = ax.bar(order, [sentiment_counts[s] for s in order],\n",
    "              color=[colors[s] for s in order], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "for bar, label in zip(bars, order):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., height + 200,\n",
    "            f'{height:,}\\n({sentiment_pct[label]:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_title('Sentiment Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Addressing Class Imbalance\n",
    "\n",
    "**Observation**: The dataset is likely imbalanced with the Positive class dominating. This is expected in product reviews where satisfied customers are more numerous.\n",
    "\n",
    "**Strategy to handle imbalance**:\n",
    "1. Use **stratified train-test split** to maintain class proportions in both sets\n",
    "2. Use **`class_weight='balanced'`** in models that support it (Logistic Regression, LinearSVC) — this automatically adjusts weights inversely proportional to class frequencies\n",
    "3. Evaluate using **macro-averaged F1-score** rather than accuracy alone, as accuracy can be misleading with imbalanced data\n",
    "4. Examine **per-class precision and recall** to ensure the model performs well on minority classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Preprocessing\n",
    "\n",
    "Text preprocessing is essential for reducing noise and improving model performance. Our pipeline:\n",
    "\n",
    "1. **Combine title + text**: The review title often contains a concise sentiment summary (e.g., \"Terrible game!\" or \"Absolutely love it!\") that adds valuable signal\n",
    "2. **Handle missing text**: Replace NaN values with empty strings to avoid errors\n",
    "3. **Clean text**: Remove HTML tags, URLs, special characters, digits, and extra whitespace\n",
    "4. **Lowercase**: Normalize case to treat \"Good\" and \"good\" as the same token\n",
    "5. **Tokenize**: Split text into individual words for further processing\n",
    "6. **Remove stopwords**: Remove common English words (the, is, at, etc.) that carry little semantic meaning\n",
    "7. **Lemmatize**: Reduce words to their base forms (e.g., \"playing\" → \"play\", \"games\" → \"game\") for better generalization\n",
    "\n",
    "**Rationale**: Each step reduces the vocabulary size and noise, allowing the model to focus on words that are truly indicative of sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing text values\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Combine title and text for richer features\n",
    "df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "print(f'Sample combined text (first review):')\n",
    "print(f'Title: {df[\"title\"].iloc[0]}')\n",
    "print(f'Text:  {df[\"text\"].iloc[0][:200]}...')\n",
    "print(f'Combined: {df[\"combined_text\"].iloc[0][:200]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Comprehensive text cleaning pipeline.\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Full preprocessing: clean, tokenize, remove stopwords, lemmatize.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the preprocessing pipeline\n",
    "sample = 'This game is ABSOLUTELY amazing!!! I love it <br> so much... http://example.com 5/5'\n",
    "print(f'Original:     {sample}')\n",
    "print(f'Cleaned:      {clean_text(sample)}')\n",
    "print(f'Preprocessed: {preprocess_text(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "# Using a sample-friendly approach — preprocess all rows\n",
    "print('Preprocessing text... This may take a few minutes.')\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "print(f'Preprocessing complete. {len(df):,} reviews processed.')\n",
    "\n",
    "# Show before and after examples\n",
    "print('\\n--- Before and After Preprocessing ---')\n",
    "for i in range(3):\n",
    "    print(f'\\nReview {i+1}:')\n",
    "    print(f'  Original:     {df[\"combined_text\"].iloc[i][:100]}...')\n",
    "    print(f'  Preprocessed: {df[\"processed_text\"].iloc[i][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exploratory Text Analysis\n",
    "\n",
    "Before building models, we explore the processed text to understand patterns across sentiment classes. This helps validate our preprocessing and reveals insights about the language used in each sentiment category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis by sentiment\n",
    "df['text_length'] = df['processed_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "order = ['Negative', 'Neutral', 'Positive']\n",
    "colors_list = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "bp = sns.boxplot(x='sentiment', y='text_length', data=df, order=order,\n",
    "                 palette=colors_list, ax=axes[0], showfliers=False)\n",
    "axes[0].set_title('Review Length by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Word Count')\n",
    "\n",
    "# Histogram\n",
    "for sent, color in zip(order, colors_list):\n",
    "    subset = df[df['sentiment'] == sent]['text_length']\n",
    "    axes[1].hist(subset, bins=50, alpha=0.5, label=sent, color=color, range=(0, 200))\n",
    "axes[1].set_title('Distribution of Review Lengths', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print('Average word count by sentiment:')\n",
    "print(df.groupby('sentiment')['text_length'].describe()[['mean', 'std', 'min', 'max']].round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words per sentiment class\n",
    "from collections import Counter\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "sentiments = ['Negative', 'Neutral', 'Positive']\n",
    "colors_map = {'Negative': '#e74c3c', 'Neutral': '#f39c12', 'Positive': '#2ecc71'}\n",
    "\n",
    "for ax, sent in zip(axes, sentiments):\n",
    "    text = ' '.join(df[df['sentiment'] == sent]['processed_text'].values)\n",
    "    word_freq = Counter(text.split())\n",
    "    top_words = word_freq.most_common(20)\n",
    "    words, counts = zip(*top_words)\n",
    "    ax.barh(range(len(words)), counts, color=colors_map[sent])\n",
    "    ax.set_yticks(range(len(words)))\n",
    "    ax.set_yticklabels(words)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f'Top 20 Words — {sent}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from text analysis**:\n",
    "- Negative reviews tend to be longer — dissatisfied customers often explain their complaints in detail\n",
    "- Positive reviews frequently use words like \"great\", \"fun\", \"love\", \"good\"\n",
    "- Negative reviews contain words like \"bad\", \"worst\", \"waste\", \"boring\"\n",
    "- These distinct word patterns suggest that bag-of-words and TF-IDF features should capture sentiment well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Engineering — Text Vectorization\n",
    "\n",
    "We convert the preprocessed text into numerical features using two methods:\n",
    "\n",
    "1. **TF-IDF (Term Frequency–Inverse Document Frequency)**:\n",
    "   - Weights words by their importance in a document relative to the corpus\n",
    "   - Common words (appearing in many reviews) get lower weight\n",
    "   - Rare, distinctive words get higher weight\n",
    "   - **Best for**: Logistic Regression and SVM, which benefit from continuous, normalized features\n",
    "\n",
    "2. **Count Vectorizer (Bag of Words)**:\n",
    "   - Simply counts word occurrences in each document\n",
    "   - **Best for**: Multinomial Naive Bayes, which models word counts as a multinomial distribution\n",
    "\n",
    "**Rationale for TF-IDF as primary vectorizer**:\n",
    "- TF-IDF generally outperforms raw counts because it downweights ubiquitous terms\n",
    "- It produces normalized features that work well with linear models\n",
    "- The `max_features=20000` limit controls dimensionality while retaining the most informative terms\n",
    "- Using both unigrams and bigrams (`ngram_range=(1,2)`) captures phrases like \"not good\" that reverse sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df['processed_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f'Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}')\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'\\nTraining set size: {len(X_train):,}')\n",
    "print(f'Testing set size:  {len(X_test):,}')\n",
    "print(f'\\nTraining set sentiment distribution:')\n",
    "print(y_train.value_counts())\n",
    "print(f'\\nTest set sentiment distribution:')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f'TF-IDF feature matrix shape (train): {X_train_tfidf.shape}')\n",
    "print(f'TF-IDF feature matrix shape (test):  {X_test_tfidf.shape}')\n",
    "print(f'Vocabulary size: {len(tfidf.vocabulary_):,} terms')\n",
    "\n",
    "# Count Vectorizer (for Naive Bayes)\n",
    "count_vec = CountVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)\n",
    "\n",
    "print(f'\\nCount Vectorizer feature matrix shape (train): {X_train_count.shape}')\n",
    "print(f'Count Vectorizer feature matrix shape (test):  {X_test_count.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Modelling\n",
    "\n",
    "We build **three classification models** using different algorithms. Each model is selected for specific strengths in text classification:\n",
    "\n",
    "| Model | Algorithm | Vectorizer | Key Strength |\n",
    "|-------|-----------|------------|-------------|\n",
    "| Model 1 | Logistic Regression | TF-IDF | Strong baseline, interpretable, probabilistic output |\n",
    "| Model 2 | Linear SVC | TF-IDF | Excellent in high-dimensional spaces, margin-based |\n",
    "| Model 3 | Multinomial Naive Bayes | Count Vectorizer | Fast, probabilistic, natural fit for word counts |\n",
    "\n",
    "**Why these three models?**\n",
    "- They represent three fundamentally different approaches: discriminative linear (LR), maximum-margin (SVM), and generative probabilistic (NB)\n",
    "- All are well-established for text classification in both academic literature and industry practice\n",
    "- They are computationally efficient for large, high-dimensional text data\n",
    "- Each uses hyperparameter tuning via GridSearchCV with 3-fold cross-validation to find optimal settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model 1: Logistic Regression with TF-IDF\n",
    "\n",
    "**Rationale for selection**:\n",
    "- Logistic Regression is one of the **strongest baselines** for text classification and frequently matches or outperforms more complex models\n",
    "- It is **highly interpretable** — we can examine feature coefficients to understand which words drive predictions\n",
    "- It provides **probabilistic outputs** (class probabilities), useful for confidence scoring\n",
    "- The regularization parameter `C` controls the trade-off between fitting the training data and generalizing to new data\n",
    "- Using `class_weight='balanced'` compensates for class imbalance by giving minority classes higher weight\n",
    "\n",
    "**Hyperparameters tuned**:\n",
    "- `C`: Inverse regularization strength — lower values = stronger regularization\n",
    "- `solver`: Algorithm for optimization — 'lbfgs' is efficient for multiclass with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "print('Training Model 1: Logistic Regression with TF-IDF')\n",
    "print('=' * 55)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "lr_params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', random_state=42, multi_class='multinomial'),\n",
    "    lr_params,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f'Best parameters: {lr_grid.best_params_}')\n",
    "print(f'Best CV F1 (macro): {lr_grid.best_score_:.4f}')\n",
    "\n",
    "# Predictions\n",
    "lr_model = lr_grid.best_estimator_\n",
    "lr_train_pred = lr_model.predict(X_train_tfidf)\n",
    "lr_test_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "print(f'\\nTraining Accuracy: {lr_train_acc:.4f}')\n",
    "print(f'Testing Accuracy:  {lr_test_acc:.4f}')\n",
    "print(f'\\nClassification Report (Test Set):')\n",
    "print(classification_report(y_test, lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model 2: Linear Support Vector Machine (LinearSVC) with TF-IDF\n",
    "\n",
    "**Rationale for selection**:\n",
    "- SVMs are **theoretically well-suited** for high-dimensional, sparse data like text features — they find the optimal hyperplane that maximizes the margin between classes\n",
    "- LinearSVC is specifically designed for **efficient large-scale** text classification\n",
    "- It often achieves **state-of-the-art performance** on text classification benchmarks\n",
    "- Unlike kernel SVMs, LinearSVC scales linearly with dataset size, making it practical for 50K reviews\n",
    "- The `class_weight='balanced'` parameter handles our class imbalance\n",
    "\n",
    "**Hyperparameters tuned**:\n",
    "- `C`: Regularization parameter — controls the penalty for misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Linear SVC\n",
    "print('Training Model 2: LinearSVC with TF-IDF')\n",
    "print('=' * 55)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svc_params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "svc_grid = GridSearchCV(\n",
    "    LinearSVC(class_weight='balanced', random_state=42),\n",
    "    svc_params,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "svc_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f'Best parameters: {svc_grid.best_params_}')\n",
    "print(f'Best CV F1 (macro): {svc_grid.best_score_:.4f}')\n",
    "\n",
    "# Predictions\n",
    "svc_model = svc_grid.best_estimator_\n",
    "svc_train_pred = svc_model.predict(X_train_tfidf)\n",
    "svc_test_pred = svc_model.predict(X_test_tfidf)\n",
    "\n",
    "svc_train_acc = accuracy_score(y_train, svc_train_pred)\n",
    "svc_test_acc = accuracy_score(y_test, svc_test_pred)\n",
    "\n",
    "print(f'\\nTraining Accuracy: {svc_train_acc:.4f}')\n",
    "print(f'Testing Accuracy:  {svc_test_acc:.4f}')\n",
    "print(f'\\nClassification Report (Test Set):')\n",
    "print(classification_report(y_test, svc_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model 3: Multinomial Naive Bayes with Count Vectorizer\n",
    "\n",
    "**Rationale for selection**:\n",
    "- Multinomial Naive Bayes is a **classic text classification algorithm** that treats documents as bags of words with multinomial frequency distributions\n",
    "- It provides a **fundamentally different approach** — a generative probabilistic model versus the discriminative models above\n",
    "- Despite its \"naive\" independence assumption, it performs **surprisingly well** on text data because high-dimensional text features tend to be approximately conditionally independent\n",
    "- It is **extremely fast** to train and predict, making it ideal for production systems\n",
    "- We use **Count Vectorizer** (instead of TF-IDF) because MultinomialNB expects raw frequency counts that follow a multinomial distribution\n",
    "\n",
    "**Hyperparameters tuned**:\n",
    "- `alpha`: Laplace smoothing parameter — prevents zero probabilities for unseen words; higher values increase smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Multinomial Naive Bayes\n",
    "print('Training Model 3: Multinomial Naive Bayes with Count Vectorizer')\n",
    "print('=' * 55)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "nb_params = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    nb_params,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "nb_grid.fit(X_train_count, y_train)\n",
    "\n",
    "print(f'Best parameters: {nb_grid.best_params_}')\n",
    "print(f'Best CV F1 (macro): {nb_grid.best_score_:.4f}')\n",
    "\n",
    "# Predictions\n",
    "nb_model = nb_grid.best_estimator_\n",
    "nb_train_pred = nb_model.predict(X_train_count)\n",
    "nb_test_pred = nb_model.predict(X_test_count)\n",
    "\n",
    "nb_train_acc = accuracy_score(y_train, nb_train_pred)\n",
    "nb_test_acc = accuracy_score(y_test, nb_test_pred)\n",
    "\n",
    "print(f'\\nTraining Accuracy: {nb_train_acc:.4f}')\n",
    "print(f'Testing Accuracy:  {nb_test_acc:.4f}')\n",
    "print(f'\\nClassification Report (Test Set):')\n",
    "print(classification_report(y_test, nb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluation\n",
    "\n",
    "We now comprehensively compare all three models across multiple metrics to select the best one. Our evaluation strategy:\n",
    "\n",
    "1. **Confusion matrices** — visualise where each model makes errors\n",
    "2. **Performance comparison table** — compare Accuracy, Precision, Recall, and F1-Score\n",
    "3. **Bar chart comparison** — visual summary of key metrics\n",
    "4. **Cross-validation** — assess model robustness and variance\n",
    "5. **Error analysis** — examine misclassified examples\n",
    "6. **Best model selection** — justified recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Confusion Matrix Visualizations\n",
    "\n",
    "Confusion matrices show the detailed breakdown of correct and incorrect predictions for each class. This reveals:\n",
    "- Which sentiment classes are easiest/hardest to classify\n",
    "- Common misclassification patterns (e.g., Neutral being confused with Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all three models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "models_info = [\n",
    "    ('Logistic Regression', lr_test_pred),\n",
    "    ('LinearSVC', svc_test_pred),\n",
    "    ('Multinomial NB', nb_test_pred)\n",
    "]\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "for ax, (name, preds) in zip(axes, models_info):\n",
    "    cm = confusion_matrix(y_test, preds, labels=labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    ax.set_title(f'{name}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.suptitle('Confusion Matrices — All Models', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance Comparison Table\n",
    "\n",
    "We compute multiple metrics for a comprehensive comparison:\n",
    "- **Accuracy**: Overall correct predictions (can be misleading with imbalanced data)\n",
    "- **Precision (macro)**: Average precision across all classes — measures false positive control\n",
    "- **Recall (macro)**: Average recall across all classes — measures false negative control\n",
    "- **F1-Score (macro)**: Harmonic mean of precision and recall — our primary metric as it balances both and treats all classes equally regardless of size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "results = {}\n",
    "model_preds = {\n",
    "    'Logistic Regression': (lr_test_pred, lr_train_acc),\n",
    "    'LinearSVC': (svc_test_pred, svc_train_acc),\n",
    "    'Multinomial NB': (nb_test_pred, nb_train_acc)\n",
    "}\n",
    "\n",
    "for name, (preds, train_acc) in model_preds.items():\n",
    "    results[name] = {\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision (macro)': precision_score(y_test, preds, average='macro'),\n",
    "        'Recall (macro)': recall_score(y_test, preds, average='macro'),\n",
    "        'F1-Score (macro)': f1_score(y_test, preds, average='macro')\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print('\\n========== MODEL PERFORMANCE COMPARISON ==========')\n",
    "print(results_df.to_string())\n",
    "print('\\nBest model by F1-Score (macro):', results_df['F1-Score (macro)'].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visual Comparison of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "metrics_to_plot = ['Test Accuracy', 'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for i, (model_name, color) in enumerate(zip(model_names, colors)):\n",
    "    values = [results[model_name][m] for m in metrics_to_plot]\n",
    "    bars = ax.bar(x + i * width, values, width, label=model_name, color=color, edgecolor='black', linewidth=0.5)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2., bar.get_height() + 0.005,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Metric', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(metrics_to_plot)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Cross-Validation for Robustness\n",
    "\n",
    "We perform 5-fold cross-validation to assess each model's stability and generalization ability. A model with low variance across folds is more reliable than one with high variance, even if the latter has a slightly higher mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison\n",
    "print('Performing 5-fold cross-validation...')\n",
    "print('=' * 55)\n",
    "\n",
    "cv_models = {\n",
    "    'Logistic Regression': (lr_model, X_train_tfidf),\n",
    "    'LinearSVC': (svc_model, X_train_tfidf),\n",
    "    'Multinomial NB': (nb_model, X_train_count)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, (model, X_data) in cv_models.items():\n",
    "    scores = cross_val_score(model, X_data, y_train, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    cv_results[name] = scores\n",
    "    print(f'{name:>25}: Mean F1 = {scores.mean():.4f} (+/- {scores.std():.4f})  | Folds: {np.round(scores, 4)}')\n",
    "\n",
    "# Visualise CV results\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "positions = range(1, len(cv_results) + 1)\n",
    "bp = ax.boxplot([cv_results[name] for name in cv_results],\n",
    "                labels=list(cv_results.keys()), patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('Cross-Validation F1-Score (Macro) Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (Macro)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Strengths and Weaknesses\n",
    "\n",
    "| Model | Strengths | Weaknesses |\n",
    "|-------|-----------|------------|\n",
    "| **Logistic Regression** | Interpretable coefficients; probabilistic output; good balance of precision/recall; handles imbalanced classes well with `class_weight` | May underperform on highly non-linear decision boundaries |\n",
    "| **LinearSVC** | Maximizes class margin for better generalization; scales well to large feature spaces; typically top performer for text | No probability estimates by default; sensitive to C parameter; less interpretable than LR |\n",
    "| **Multinomial NB** | Extremely fast training/prediction; works well with small training data; natural probabilistic model for word counts | Strong independence assumption may hurt; struggles with complex feature interactions; cannot use `class_weight` |\n",
    "\n",
    "**Key observations**:\n",
    "- The Neutral class is hardest to classify for all models — this is expected because neutral reviews use language that overlaps with both positive and negative sentiments\n",
    "- Positive class has highest recall due to its larger representation in the data\n",
    "- The gap between training and testing accuracy indicates whether a model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Error Analysis\n",
    "\n",
    "Examining misclassified examples helps us understand model limitations and identify potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis using the best model\n",
    "# Determine best model for error analysis\n",
    "best_model_name = results_df['F1-Score (macro)'].idxmax()\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_preds = lr_test_pred\n",
    "elif best_model_name == 'LinearSVC':\n",
    "    best_preds = svc_test_pred\n",
    "else:\n",
    "    best_preds = nb_test_pred\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "test_df = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'actual': y_test.values,\n",
    "    'predicted': best_preds\n",
    "})\n",
    "\n",
    "misclassified = test_df[test_df['actual'] != test_df['predicted']]\n",
    "correct = test_df[test_df['actual'] == test_df['predicted']]\n",
    "\n",
    "print(f'Best model for error analysis: {best_model_name}')\n",
    "print(f'Total test samples: {len(test_df):,}')\n",
    "print(f'Correctly classified: {len(correct):,} ({len(correct)/len(test_df)*100:.1f}%)')\n",
    "print(f'Misclassified: {len(misclassified):,} ({len(misclassified)/len(test_df)*100:.1f}%)')\n",
    "\n",
    "# Misclassification patterns\n",
    "print(f'\\n--- Misclassification Patterns ---')\n",
    "misclass_patterns = misclassified.groupby(['actual', 'predicted']).size().reset_index(name='count')\n",
    "misclass_patterns = misclass_patterns.sort_values('count', ascending=False)\n",
    "print(misclass_patterns.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample misclassified reviews\n",
    "print('\\n--- Sample Misclassified Reviews ---\\n')\n",
    "for actual_label in ['Negative', 'Neutral', 'Positive']:\n",
    "    subset = misclassified[misclassified['actual'] == actual_label].head(2)\n",
    "    if len(subset) > 0:\n",
    "        print(f'Actual: {actual_label}')\n",
    "        for _, row in subset.iterrows():\n",
    "            text_preview = row['text'][:120] + '...' if len(row['text']) > 120 else row['text']\n",
    "            print(f'  Predicted: {row[\"predicted\"]} | Text: {text_preview}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Best Model Selection\n",
    "\n",
    "Based on our comprehensive evaluation, we now select the best model with clear justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection summary\n",
    "print('=' * 60)\n",
    "print('           FINAL MODEL SELECTION SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "best_name = results_df['F1-Score (macro)'].idxmax()\n",
    "best_row = results_df.loc[best_name]\n",
    "\n",
    "print(f'\\n>>> BEST MODEL: {best_name} <<<\\n')\n",
    "print(f'Test Accuracy:     {best_row[\"Test Accuracy\"]:.4f}')\n",
    "print(f'Precision (macro): {best_row[\"Precision (macro)\"]:.4f}')\n",
    "print(f'Recall (macro):    {best_row[\"Recall (macro)\"]:.4f}')\n",
    "print(f'F1-Score (macro):  {best_row[\"F1-Score (macro)\"]:.4f}')\n",
    "\n",
    "print(f'\\n--- Ranking by F1-Score (Macro) ---')\n",
    "ranking = results_df['F1-Score (macro)'].sort_values(ascending=False)\n",
    "for i, (model, score) in enumerate(ranking.items(), 1):\n",
    "    marker = ' <<<' if model == best_name else ''\n",
    "    print(f'  {i}. {model}: {score:.4f}{marker}')\n",
    "\n",
    "print(f'\\n--- Cross-Validation Stability ---')\n",
    "for name, scores in cv_results.items():\n",
    "    stability = 'High' if scores.std() < 0.01 else 'Medium' if scores.std() < 0.02 else 'Low'\n",
    "    print(f'  {name}: std = {scores.std():.4f} ({stability} stability)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Best Model Selection\n",
    "\n",
    "The best model is selected based on the following criteria, ranked by importance:\n",
    "\n",
    "1. **F1-Score (Macro)** — Our primary metric because it balances precision and recall equally across all sentiment classes, regardless of class size. This ensures the model performs well on minority classes (Negative and Neutral), not just the majority Positive class.\n",
    "\n",
    "2. **Cross-Validation Stability** — A model that performs consistently across different data splits is more trustworthy for production deployment than one with high variance.\n",
    "\n",
    "3. **Generalization Gap** — A small gap between training and testing accuracy indicates the model is not overfitting to the training data.\n",
    "\n",
    "4. **Practical Considerations**:\n",
    "   - **Logistic Regression**: Offers the best interpretability — businesses can understand *why* a review is classified as positive or negative by examining the top contributing words. It also provides calibrated probability estimates.\n",
    "   - **LinearSVC**: Typically achieves the highest raw accuracy for text classification but lacks native probability outputs.\n",
    "   - **Multinomial NB**: Fastest inference time, ideal if speed is the primary concern.\n",
    "\n",
    "The selected model provides the best overall balance of accuracy, robustness, and practical utility for a business sentiment analysis system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Business Recommendation\n",
    "\n",
    "**How can businesses use this sentiment classification model?**\n",
    "\n",
    "1. **Automated Review Monitoring**: Deploy the model to automatically classify incoming reviews into Positive, Neutral, and Negative categories in real-time. This enables product managers to quickly identify and respond to negative feedback.\n",
    "\n",
    "2. **Customer Satisfaction Tracking**: Aggregate sentiment predictions over time to track customer satisfaction trends. A sudden increase in negative sentiment could indicate a product defect, a bad update, or a service issue that needs immediate attention.\n",
    "\n",
    "3. **Product Improvement Prioritization**: Analyse the most common words and phrases in negative reviews (using the model's interpretable features) to identify specific issues customers complain about — e.g., \"lag\", \"crash\", \"overpriced\".\n",
    "\n",
    "4. **Review Summarization**: Filter and surface the most informative reviews for each sentiment class to help potential buyers make informed decisions.\n",
    "\n",
    "5. **Competitive Analysis**: Apply the model to competitor product reviews to benchmark sentiment and identify areas of competitive advantage or weakness.\n",
    "\n",
    "**Limitations to be aware of**:\n",
    "- The model is trained on video game reviews and may not generalize well to other product categories\n",
    "- Sarcasm and irony (e.g., \"Great, another broken game\") may be misclassified\n",
    "- The Neutral class remains the most challenging to predict accurately\n",
    "- Model performance should be monitored and retrained periodically as review language evolves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Citation\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "Hou, Y., Li, J., He, Z., Yan, A., Chen, X., & McAuley, J. (2024). *Bridging Language and Items for Retrieval and Recommendation*. arXiv preprint arXiv:2403.03952."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "To export this notebook as HTML for submission:\n",
    "\n",
    "1. In Jupyter Notebook: **File → Download as → HTML (.html)**\n",
    "2. In JupyterLab: **File → Export Notebook As → HTML**\n",
    "3. Via command line: `jupyter nbconvert --to html SentimentClassificationStarter.ipynb`\n",
    "\n",
    "Ensure all cells have been run and outputs are visible before exporting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}